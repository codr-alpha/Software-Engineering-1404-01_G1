Based on the **Diagrams (PDF 1)** and the **Software Design Document (PDF 2)**, here is a comprehensive technical specification of the TOEFL AI Evaluation Platform.

This document integrates the visual structural logic with the descriptive architectural decisions provided in the SDD text.

---

# System Architecture & Technical Specification

## 1. System Overview
**Name:** TOEFL AI Evaluation Microservice
**Architecture:** 3-Layered Architecture (Controller-Service-Repository)
**Deployment:** Containerized (Docker)
**Primary Responsibility:** Automated evaluation of TOEFL Writing and Speaking tasks using LLMs (Large Language Models) and ASR (Automatic Speech Recognition).

---

## 2. Component Design (Class Level)
The system follows the **Separation of Concerns** principle, divided into three distinct layers as detailed in the Class Diagram (PDF 1) and the Class Design section (PDF 2).

### A. Controller Layer (Entry Point)
*   **`EvaluationController`**: Handles incoming HTTP requests for writing/speaking submissions and history retrieval. It performs basic DTO validation before passing data to the Service layer.
*   **`AuthController`**: Manages user authentication.
    *   **Methods:** `login()`, `verifyPass()`, `generateJWT()`.
    *   **Logic:** Validates credentials against the database hash and issues a time-bound **JWT** for session management.

### B. Service & Domain Layer (Business Logic)
This is the core processing unit.
*   **`EvaluationService`**: The orchestrator. It directs traffic to specific evaluators (Writing vs. Speaking) and handles data persistence.
*   **`WritingEvaluator`**:
    *   **Validation:** Checks word count constraints internally (`validateLength`).
    *   **Logic:** Constructs prompts by combining the User's text + Question Prompt + Rubric. Parses the LLM's JSON response into a `WritingScore` object.
*   **`SpeakingEvaluator`**:
    *   **Logic:** Orchestrates the multi-step process: Audio $\to$ ASR $\to$ Transcript $\to$ LLM $\to$ Score.
    *   **Checks:** Verifies if speech was actually detected in the audio file before sending to LLM to save costs.
*   **`AnalyticsService`**:
    *   **Logic:** Pure computation. Calculates averages, trends, and improvement rates based on raw data fetched by the repository. It separates "Data Retrieval" from "Data Analysis."

### C. Infrastructure & Repository Layer (Data & External APIs)
*   **`ResultRepository`**: Interface for Database operations (PostgreSQL).
*   **`LLMClient`**: Wrapper for the AI Provider API (e.g., OpenAI). Handles API keys and prompt transmission.
*   **`ASRClient`**: Wrapper for the Speech-to-Text service. Converts `.wav` files to text.

---

## 3. Database Design (Schema)
The system uses a Relational Database (PostgreSQL) optimized for normalization and flexibility.

### Key Entities
1.  **`users`**: Stores auth details (`password_hash`, `email`) and roles (`admin`, `student`).
2.  **`questions`**: The bank of practice questions.
    *   **Features:** Distinguishes between `independent` and `integrated` tasks.
3.  **`evaluations`**: The central transaction table.
    *   **Polymorphism:** Stores both Writing (`submitted_text`) and Speaking (`audio_path`) results.
    *   **Optimization:** Does **not** store binary audio files (BLOBs). It stores file paths (links to S3/MinIO) to maintain DB performance.
4.  **`detailed_scores`**:
    *   **Relationship:** One Evaluation has Many Detailed Scores (1:N).
    *   **Purpose:** Stores breakdown scores (e.g., Grammar, Fluency, Coherence) separately.
    *   **Design Consideration:** This allows the grading rubric to change (adding/removing criteria) without needing `ALTER TABLE` commands on the main evaluations table.
5.  **`api_logs`**:
    *   **Purpose:** Non-functional monitoring. Tracks `latency_ms` and `status_code` to monitor Service Level Agreements (SLA) and system health.

---

## 4. Functional Workflows (The "Jobs")

### Job 1: Writing Evaluation (UC-01)
**Inputs:** Text Essay, Question ID, User ID.
**Process:**
1.  **Validation:** Client checks for empty text. Server (`WritingEvaluator`) checks word count limits.
2.  **Prompt Engineering:** System constructs a prompt: *[Essay Text] + [ETS Rubric]*.
3.  **AI Analysis:** Request sent to `LLMClient`.
4.  **Parsing:** The LLM returns a JSON containing specific criteria scores and textual feedback.
5.  **Persistence:** `EvaluationService` saves the summary to `evaluations` and the breakdown to `detailed_scores`.

### Job 2: Speaking Evaluation (UC-02)
**Inputs:** Audio File (.wav), Question ID, User ID.
**Process:**
1.  **Validation:** Check file size limit.
2.  **ASR Processing:** Audio is sent to `ASRClient`.
    *   *Decision Node:* If no speech is detected (silence/noise), the process aborts with an error (saving LLM costs).
3.  **Transcription:** Audio is converted to text (`transcript_text`).
4.  **Hybrid Scoring:**
    *   **Fluency:** Calculated from ASR metadata (confidence, pauses).
    *   **Content/Grammar:** Calculated by sending the *Transcript* + *Rubric* to `LLMClient`.
5.  **Storage:** Audio file is uploaded to object storage (Cloud); only the **path** is saved in the DB.

### Job 3: Progress Analytics (UC-03)
**Inputs:** User ID.
**Process:**
1.  **Retrieval:** `EvaluationController` requests history. `ResultRepository` fetches raw records.
2.  **Empty State Handling:** If no records exist, a specific "Welcome/Start Test" JSON is returned.
3.  **Analytics Phase:** If data exists, `AnalyticsService` computes:
    *   Average scores.
    *   Trend lines (improvement over time).
4.  **Visualization:** The Frontend renders a Chart based on this processed JSON.

---

## 5. Technical Design Considerations
The SDD explicitly outlines why certain engineering decisions were made:

*   **UUIDs:** All Primary Keys (`user_id`, `log_id`) use UUIDs instead of Integers. This is to support future database sharding or data merging without ID collision.
*   **JSON Handling:** The AI output is strictly enforced as JSON to allow the backend to parse specific scores (e.g., "Grammar: 4.0") programmatically rather than just displaying raw text to the user.
*   **API Management (Admin):** An internal `api_logs` table allows Admins (via `Admin Panel`) to monitor if external AI services are down (`status_code`) or slow (`latency_ms`).